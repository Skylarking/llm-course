{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 一、简介\n",
    "- parameter efficient fine tuning翻译为参数高效率微调，和全参数微调相对应\n",
    "- 参考：https://zhuanlan.zhihu.com/p/707573525\n",
    "- LoRA参考：https://zhuanlan.zhihu.com/p/688993851\n",
    "- 代码可参考hf（除了下面介绍的，还有其他方法）：https://github.com/huggingface/peft\n",
    "## 1. 为什么需要PEFT\n",
    "- LLM模型参数量越来越大，训练一次需要耗费几亿美金。所以大部分都训练不起\n",
    "- 因此很多小公司或者个人在开源LLM上微调，但是即使微调，也是大部分都负担不起的\n",
    "- PEFT能节省大量微调所需资源，并且效果也很好\n",
    "- 这种方式冻结大部分预训练参数，只微调一小部分参数"
   ],
   "id": "3f2f1c875f47b9e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 二、基于Adapter Tuning的方法\n",
    "## 1. 原始架构\n",
    "- 针对特定下游任务，在原始Transformer层中添加一个Adapter层\n",
    "- 微调时固定原始所有参数，只训练新增的Adapter和LayerNorm的参数\n",
    "<p align=\"center\">\n",
    "  <img src=\"./imgs/v2-42bb08dd821b120db744e1b82710ba80_b.png\" width=\"300\">\n",
    "  <span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>\n",
    "  <img src=\"./imgs/v2-89829b9356b7ed9fe826d9c15e215b7c_b.png\" width=\"300\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Image 1: 总体架构</em>\n",
    "  <span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>\n",
    "  <em>Image 2: Adapter内的架构</em>\n",
    "</p>"
   ],
   "id": "3aa5945ff6b265e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Adapter Fusion\n",
    "- 多任务微调时，针对每个任务分别用一个Adapter微调。\n",
    "- 出煤Adapter还额外增加Adapter Fusion模块\n",
    "- 在微调某一个任务时，需要固定原始模型参数和其他Adapter的参数；Adapter Fusion\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./imgs/v2-2ce248ede2322257052f4f114f2735e2_720w.jpg\" width=\"300\">\n",
    "  <span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>\n",
    "  <img src=\"./imgs/v2-c9e5083409dab6db80a852eeee2bbbff_720w.jpg\" width=\"300\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Image 1: 总体架构</em>\n",
    "  <span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>\n",
    "  <em>Image 2: Adapter内的架构</em>\n",
    "</p>"
   ],
   "id": "1f7c4f5834bd508a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc2f1c4aa25de983"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Adapter-Drop\n",
    "- 在Adapter fusion基础上，由于增加太多额外层，推理效率下降。\n",
    "- 因此，在较低层的transformer就是用原始网络，不额外增加层\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./imgs/v2-1a4d788eb753e8a3ee681acc1840e00b_720w.jpg\" width=\"500\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Image 1: 总体架构</em>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./imgs/v2-e3afa0803f7c606a0da73615e89dee9f_720w.jpg\" width=\"500\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Image 1: 不同任务使用不同的prefix</em>\n",
    "</p>"
   ],
   "id": "164cde95c48a1a50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 三、基于soft-prompt的方法\n",
    "- hard prompt就是人为手工构造的prompt，构建好prompt后，再prompt数据上做全量微调\n",
    "- soft prompt是\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./imgs/v2-f0f73329bf1e086f3545ac4d4283f398_720w.jpg\" width=\"800\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Image 1: 微调（fine-tuning）、prompt微调（soft-prompt），prompt设计（hard prompt）</em>\n",
    "</p>\n"
   ],
   "id": "5ee2756831ffc2b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Prefix-tuning\n",
    "- 在input前面（第一层前面）添加一个可以训练的prefix；在之后的每一层k、v前面添加prefix\n",
    "- 不同任务使用不同的prefix，相同任务使用同一个prefix\n",
    "- 在微调阶段，固定pretrained model参数，训练这个prefix\n",
    "- 为了防止不稳定，不是直接训练prefix，而是训练一个MLP，这个MLP再生成prefix\n",
    "- 部署时，直接抛弃MLP，只需要保留prefix\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./imgs/v2-ff30ca83d5e2bcef2dc1e400e7bfda10_720w.jpg\" width=\"800\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Image 1: 总体架构</em>\n",
    "</p>"
   ],
   "id": "e1e651238691e8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Prompt-tuning\n",
    "- 和prefix-tuning唯一不同的就是：直接训练prefix\n",
    "- 因为研究者发现，在参数量很大时，稳定性的问题不存在了\n",
    "- 并且prefix参数越多，效果越好\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./imgs/v2-581ca4f87575033f2e31de7e35f4d7e9_720w.jpg\" width=\"800\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Image 1: 左：fine-tuning全量微调；右：prompt-tuning</em>\n",
    "</p>\n",
    "\n",
    "### (1). prompt-tuning实现"
   ],
   "id": "cd06b5d879112e62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SoftEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                wte: nn.Embedding,\n",
    "                n_tokens: int = 10, \n",
    "                random_range: float = 0.5,\n",
    "                initialize_from_vocab: bool = True):\n",
    "        \"\"\"appends learned embedding to \n",
    "        Args:\n",
    "            wte (nn.Embedding): original transformer word embedding\n",
    "            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n",
    "            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n",
    "            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n",
    "        \"\"\"\n",
    "        super(SoftEmbedding, self).__init__()\n",
    "        self.wte = wte\n",
    "        self.n_tokens = n_tokens\n",
    "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
    "                                                                               n_tokens, \n",
    "                                                                               random_range, \n",
    "                                                                               initialize_from_vocab))\n",
    "            \n",
    "    def initialize_embedding(self, \n",
    "                             wte: nn.Embedding,\n",
    "                             n_tokens: int = 10, \n",
    "                             random_range: float = 0.5, \n",
    "                             initialize_from_vocab: bool = True):\n",
    "        \"\"\"initializes learned embedding\n",
    "        Args:\n",
    "            same as __init__\n",
    "        Returns:\n",
    "            torch.float: initialized using original schemes\n",
    "        \"\"\"\n",
    "        if initialize_from_vocab:\n",
    "            return self.wte.weight[:n_tokens].clone().detach()\n",
    "        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)\n",
    "            \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"run forward pass\n",
    "        Args:\n",
    "            tokens (torch.long): input tokens before encoding\n",
    "        Returns:\n",
    "            torch.float: encoding of text concatenated with learned task specifc embedding\n",
    "        \"\"\"\n",
    "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
    "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
    "        return torch.cat([learned_embedding, input_embedding], 1)"
   ],
   "id": "b7b0ddd61b5bc36d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoConfig, AdamW, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "n_tokens = 20\n",
    "initialize_from_vocab = True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nezha-base-wwm\")\n",
    "config = AutoConfig.from_pretrained(\"nezha-base-wwm\", num_labels=num_class)\n",
    "config.output_hidden_states = True  # 需要设置为true才输出\n",
    "model = AutoModel.from_pretrained(model_path, config=config)\n",
    "s_wte = SoftEmbedding(model.get_input_embeddings(), \n",
    "                      n_tokens=n_tokens, \n",
    "                      initialize_from_vocab=initialize_from_vocab)\n",
    "model.set_input_embeddings(s_wte)\n",
    "inputs = tokenizer(\"May the force be\", return_tensors=\"pt\")\n",
    "\n",
    "# need to pad attention_mask and input_ids to be full seq_len + n_learned_tokens\n",
    "# even though it does not matter what you pad input_ids with, it's just to make HF happy\n",
    "inputs['input_ids'] = torch.cat([torch.full((1,n_tokens), 50256), inputs['input_ids']], 1)\n",
    "inputs['attention_mask'] = torch.cat([torch.full((1,n_tokens), 1), inputs['attention_mask']], 1)\n",
    "\n",
    "outputs = model(**inputs)"
   ],
   "id": "b91e4dfadc0aecd9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. P-tuning\n",
    "- 在prompt-tuning的基础上重新添加了LSTM-MLP的网络编码soft-prompt\n",
    "- 相当于重写了一个可学习embedding层\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"./imgs/f2a641f10fe24cb8b3775e103d35f51f~tplv-k3u1fbpfcp-zoom-in-crop-mark_1512_0_0_0.jpg\" width=\"1200\">\n",
    "</p>\n",
    "<p align=\"left\">\n",
    "  <em>Image 1: 左：相当于hard prompt；右：p-tuning</em>\n",
    "</p>"
   ],
   "id": "97ee1949db36ce18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 四、 基于LoRA（Low-Rank Adaptation）的方法\n",
    "- Adapter-tuning缺点：额外增加了计算延迟，因为不同层是串行的。\n",
    "- Prompt-tuning缺点：并不好训练、微调后的性能和可训练参数数量不成正相关；prefix会占用token数，使得输出token数减少\n",
    "\n",
    "## 1. LoRA\n",
    "- LoRA（Low-Rank Adaptation）：和Adapter-tuning类似，都是新增加参数来微调。但是使用一个旁路（而不是串行）\n",
    "- LoRA的思想是：目前的网络是过参数化（over-parameter）的，即weights矩阵实际上是一个低秩矩阵（大部分行/列向量可以有少部分的行/列向量线性表出），所以其实不需要那么多参数。因此将weights矩阵m×n拆分成m×r和r×n的两个小矩阵相乘。r就是一个内在秩。\n",
    "- 微调时，原始参数固定，只训练新增的参数。并且将最终两个输出相加\n",
    "- 原始论文只在attention上加了LoRA\n",
    "- 另外需要特别说明的是：\n",
    "    - LoRA的参数初始化时，A或者B必须某一个为全0，这样保证第一轮参数更新时，输出和原始模型输出一致。\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./imgs/v2-840b7b18c8e5a4a61c7e625c550ed013_720w.jpg\" width=\"500\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Image 1: LoRA</em>\n",
    "</p>\n"
   ],
   "id": "348fbe7b66d9ab29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. QLoRA（Quantilization Low-Rank Adaptation）\n",
    "- 即量化和LoRA的结合，在微调和部署时，使用低精度"
   ],
   "id": "58a5fa7148744d8c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

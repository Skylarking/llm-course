{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 一、简介\n",
    "- 数据集构建参考：(https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2)\n",
    "- 微调参考：(https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1)\n",
    "## 1.什么是Instruction Tuning\n",
    "- 这是post-training的一部分，即利用instructions（命令）来微调模型，使得模型能够和对齐人类的提问形式。\n",
    "- 训练方式和pre-training是一样，区别在于数据集的不同。pre-training是不用构建数据的，因为他是自监督学习（自回归）。但是在这一步我们需要构建数据集，特别是构建高质量的数据集，这一步对后面实现chat能力至关重要\n",
    "- 数据集的形式为问答对（instruction-answer pair），如下两种示例:  \n",
    "\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td>\n",
    "            <font size=6> Instruction </font> <br> Explain the concept of a bubble sort algorithm to a non-technical audience.\n",
    "            </td>\n",
    "            <td>\n",
    "            <font size=6> Answer </font> <br> A bubble sort algorithm is a type of sorting algorithm that is used to sort elements in an array. It works by looking at each element of the array and comparing it to the next element. If the first element is bigger than the second element, they are swapped. This process is repeated until the whole array is sorted. This type of sorting is one of the simplest sorting algorithms, but it can be slow if the array has many elements.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "            <font size=6> Instruction </font> <br> Make the second sentence shorter. <br> <font size=6> Context </font> <br> Winter is usually the coldest season of the year. Snow is a common element during winter.\n",
    "            </td>\n",
    "            <td>\n",
    "            <font size=6> Answer </font> <br> Winter is the coldest season, often accompanied by snow.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </table>\n",
    "\n",
    "目前已经有的高质量instruction dataset有手工构建和自动构建的。其中像[Flan Collection](https://github.com/google-research/FLAN)，[Dolly15k dataset](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)是手工构建的，而像[Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html)是利用LLMs构建的。还有开源社区提供的一些教学数据集[OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)、[Open-Platypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)、[openhermes](https://huggingface.co/datasets/teknium/openhermes)。\n",
    "\n",
    "我们使用Alpaca dataset，包括如何预处理数据集、格式化、和微调LLM"
   ],
   "id": "b67580f226871b2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. 什么是Alpaca Dataset？\n",
    "这是一个利用OpenAI davinci模型和Llama生成的数据集，。拥有许多种类的指令，包括邮件写作、社交媒体、生产力工具。构建数据集的pipeline如下图所示：  \n",
    "    <img src=./imgs/f8eba1d5.png>  \n",
    "\n",
    "\n",
    "这已经是一个老的版本了。现在我们使用更新的版本，是用GPT-4生成的。"
   ],
   "id": "d992d360abd42970"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 二、Alpaca-GPT4 Dataset\n",
    "- 是一个json文件，连接如下[Alpaca-GPT4 Dataset](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data.json)\n",
    "- 包含52K条数据\n",
    "- 是通过GPT-4生成的\n",
    "\n",
    "样例数据如下：\n",
    "    ```\n",
    "    instruction: str, describes the task the model should perform. \n",
    "                      Each of the 52K instructions is unique.\n",
    "    input:       str, optional context or input for the task.\n",
    "    output:      str, the answer to the instruction as generated by GPT-4.\n",
    "    \n",
    "    ```\n",
    "    \n",
    "下面加载数据集，采用将数据夹菜成W&B的形式，方便快速加载"
   ],
   "id": "f1f4fc69046ab45b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T08:19:02.913812Z",
     "start_time": "2024-08-01T08:19:02.875569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# 获取当前工作目录\n",
    "current_directory = os.getcwd()\n",
    "print(\"当前工作目录:\", current_directory)\n",
    "\n",
    "# 设置新的工作目录\n",
    "new_directory = \"/mnt/d/lib/llm/llm-course\"\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# 再次获取当前工作目录，确认是否更改成功\n",
    "current_directory = os.getcwd()\n",
    "print(\"新的工作目录:\", current_directory)"
   ],
   "id": "b87be3ed5a34dc7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: /home/liangxianbing\n",
      "新的工作目录: /mnt/d/code/llm/llm-course\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T08:53:39.204493Z",
     "start_time": "2024-08-01T08:53:25.299302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import wandb\n",
    "\n",
    "\n",
    "with open(\"./mynotes/data/alpaca_dataset/alpaca_gpt4_data.json\", \"r\") as f:\n",
    "    alpaca = json.load(f)\n",
    "\n",
    "\n",
    "with wandb.init(project=\"alpaca_ft\"):\n",
    "    at = wandb.Artifact(\n",
    "        name=\"alpaca_gpt4\", \n",
    "        type=\"dataset\",\n",
    "        description=\"A GPT4 generated Alpaca like dataset for instruction finetunning\",\n",
    "        metadata={\"url\":\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\"},\n",
    "    )\n",
    "    at.add_file(\"./mynotes/data/alpaca_dataset/alpaca_gpt4_data.json\")\n",
    "    \n",
    "    # table\n",
    "    table = wandb.Table(columns=list(alpaca[0].keys()))\n",
    "    for row in alpaca:\n",
    "        table.add_data(*row.values())"
   ],
   "id": "6e8c971cc4b61bb8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mskyl4rking\u001B[0m (\u001B[33mskyl4rking-fudan-university\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/code/llm/llm-course/wandb/run-20240801_165326-sk7zc6r9</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/sk7zc6r9' target=\"_blank\">hopeful-darkness-1</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/sk7zc6r9' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/sk7zc6r9</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac91a38675f648fca8ea41119a22dd88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hopeful-darkness-1</strong> at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/sk7zc6r9' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/sk7zc6r9</a><br/> View project at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240801_165326-sk7zc6r9/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.Tokenization",
   "id": "45fc32f34c40e6f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T08:57:01.135293Z",
     "start_time": "2024-08-01T08:57:00.858197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"./mynotes/data/alpaca_dataset/alpaca_gpt4_data.json\", \"r\") as f:\n",
    "    alpaca = json.load(f)\n",
    "\n",
    "\n",
    "print(len(alpaca))\n",
    "\n",
    "\n",
    "one_row = alpaca[232]\n",
    "print(one_row)"
   ],
   "id": "fecec7f9ab451335",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52002\n",
      "{'instruction': 'Sort the following list in alphabetical order.', 'input': 'Camouflage, Furniture, Plaster', 'output': 'Camouflage, Furniture, Plaster sorted in alphabetical order:\\nCamouflage, Furniture, Plaster'}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (1). 定义将数据格式化的函数（就是生成prompt）",
   "id": "d3ff5fdbfea1953d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T08:59:50.123699Z",
     "start_time": "2024-08-01T08:59:50.119642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "\n",
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "row = alpaca[232]\n",
    "print(prompt_input(row))"
   ],
   "id": "c75ffdd43a886f12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (2). 构建所有数据生成prompt的函数",
   "id": "e8e6afe7255c993f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:00:25.183685Z",
     "start_time": "2024-08-01T09:00:25.148003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_prompt(row):\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)\n",
    "\n",
    "\n",
    "prompts = [create_prompt(row) for row in alpaca]  # all LLM inputs are here\n",
    "print(prompts[232])"
   ],
   "id": "40a0c718c705fc8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (3). 定义输出（即label），只需要提那家一个eos即可",
   "id": "6ec18274124c7b40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:02:21.941568Z",
     "start_time": "2024-08-01T09:02:21.918722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EOS_TOKEN = \"</s>\"\n",
    "outputs = [row['output'] + EOS_TOKEN for row in alpaca]\n",
    "print(outputs[232])"
   ],
   "id": "b660cc7bdbc15144",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camouflage, Furniture, Plaster sorted in alphabetical order:\n",
      "Camouflage, Furniture, Plaster</s>\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (4).保存数据集",
   "id": "689681a0c80eb9fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:04:29.809239Z",
     "start_time": "2024-08-01T09:04:29.774012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = [{\"prompt\":s, \"output\":t, \"example\": s+t} for s, t in zip(prompts, outputs)]\n",
    "print(dataset[232]['prompt'])\n",
    "print('____________________________________________________________________________________________')\n",
    "print(dataset[232]['output'])\n",
    "print('____________________________________________________________________________________________')\n",
    "print(dataset[232]['example'])"
   ],
   "id": "c692a8118f5a56d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "\n",
      "____________________________________________________________________________________________\n",
      "Camouflage, Furniture, Plaster sorted in alphabetical order:\n",
      "Camouflage, Furniture, Plaster</s>\n",
      "____________________________________________________________________________________________\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "Camouflage, Furniture, Plaster sorted in alphabetical order:\n",
      "Camouflage, Furniture, Plaster</s>\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### (5). 进行Tokenizer\n",
    "- 可以看到pad了3个2"
   ],
   "id": "6d3df24b5688b608"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:16:48.939641Z",
     "start_time": "2024-08-01T09:16:48.624535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.encode(\"My experiments are going strong!\", padding='max_length', max_length=10)\n",
    "\n",
    "# 也可以直接变成torch Tensor\n",
    "# tokenizer.encode(\"My experiments are going strong!\", \n",
    "#                  padding='max_length', \n",
    "#                  max_length=10,\n",
    "#                  return_tensors=\"pt\")"
   ],
   "id": "35af52963b3385f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1619, 15729, 526, 2675, 4549, 29991, 2, 2, 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. 处理成数据集，并上传到Wandb\n",
    "### (1). 切验证集和训练集"
   ],
   "id": "8bf5868bc8019f95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:20:38.689143Z",
     "start_time": "2024-08-01T09:19:56.298086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "random.shuffle(dataset)  # shuffle inplace\n",
    "\n",
    "\n",
    "train_dataset = dataset[:-1000]\n",
    "eval_dataset = dataset[-1000:]\n",
    "\n",
    "\n",
    "train_table = wandb.Table(dataframe=pd.DataFrame(train_dataset))\n",
    "eval_table  = wandb.Table(dataframe=pd.DataFrame(eval_dataset))\n",
    "\n",
    "\n",
    "with wandb.init(project=\"alpaca_ft\", job_type=\"split_data\"):\n",
    "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})\n"
   ],
   "id": "d52978b8a0c7cda9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/code/llm/llm-course/wandb/run-20240801_171959-jrw3yxl6</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/jrw3yxl6' target=\"_blank\">lyric-fog-2</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/jrw3yxl6' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/jrw3yxl6</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='114.333 MB of 114.344 MB uploaded\\r'), FloatProgress(value=0.9999024004070258, max…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f35d00e0e544a2a912fc40969cf2744"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-fog-2</strong> at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/jrw3yxl6' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/jrw3yxl6</a><br/> View project at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240801_171959-jrw3yxl6/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 方案一：将多个样本组合成一个长序列\n",
    "长序列如下图所示，将多个样本变成一个含有1k的长序列\n",
    "<img src=./imgs/d9f4c0c2.png width=auto height=200>"
   ],
   "id": "cb721f0302cd5b0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:26:48.181417Z",
     "start_time": "2024-08-01T09:26:43.812849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_seq_len = 1024\n",
    "\n",
    "\n",
    "def pack(dataset, max_seq_len=1024):\n",
    "    tkds_ids = tokenizer([s[\"example\"] for s in dataset])[\"input_ids\"]  # 获取所有样本的token_id\n",
    "    \n",
    "    # 将dataset中所有样本token_id拼接到一起\n",
    "    all_token_ids = []\n",
    "    for tokenized_input in tkds_ids:\n",
    "        all_token_ids.extend(tokenized_input + [tokenizer.eos_token_id])    \n",
    "    \n",
    "    # 每段切1024个token，并生成对应label\n",
    "    packed_ds = []\n",
    "    for i in range(0, len(all_token_ids), max_seq_len+1):\n",
    "        input_ids = all_token_ids[i : i + max_seq_len+1]\n",
    "        if len(input_ids) == (max_seq_len+1):\n",
    "            packed_ds.append({\"input_ids\": input_ids[:-1], \"labels\": input_ids[1:]})  # < --- ‼️ ⛔️\n",
    "\t    # if you use the model.output.loss you don't need to shift, it is done for you!\n",
    "    return packed_ds\n",
    "\n",
    "\n",
    "train_ds_packed = pack(train_dataset)\n",
    "eval_ds_packed = pack(eval_dataset)\n",
    "print(len(train_ds_packed[0][\"input_ids\"]), len(train_ds_packed[0][\"labels\"]))\n",
    "print(len(train_ds_packed[44][\"input_ids\"]), len(train_ds_packed[44][\"labels\"]))"
   ],
   "id": "f35016a1b2b61dc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 1024\n",
      "1024 1024\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 方案二：将每个样本pad成同样长度\n",
    "- 可以选择最长长度pad\n",
    "- 或者选择一个最大值来pad\n",
    "两种方案如下图所示：  \n",
    "<img src=./imgs/ff512cfb.png width=auto height=200>"
   ],
   "id": "aba42ff2b8cf6cc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:34:13.366153Z",
     "start_time": "2024-08-01T09:34:13.360922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = tokenizer([\"My experiments are going strong!\", \n",
    "           \"I love Llamas\"], \n",
    "          padding='longest',\n",
    "          return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "b = tokenizer([\"My experiments are going strong!\", \n",
    "           \"I love Llamas\"], \n",
    "          # padding='max_length', \n",
    "          padding='max_length',\n",
    "          max_length=10,\n",
    "          return_tensors=\"pt\")\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ],
   "id": "d7ef51e4d83e9ba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  1619, 15729,   526,  2675,  4549, 29991],\n",
      "        [    1,   306,  5360,   365,  5288,   294,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n",
      "{'input_ids': tensor([[    1,  1619, 15729,   526,  2675,  4549, 29991,     2,     2,     2],\n",
      "        [    1,   306,  5360,   365,  5288,   294,     2,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (2). 保存数据到W&B（方案一）",
   "id": "367e79605021ad25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:36:28.267918Z",
     "start_time": "2024-08-01T09:35:40.304313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "def save_jsonl(data, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for entry in data:\n",
    "            json.dump(entry, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "# dump everything to jsonl files\n",
    "save_jsonl(train_ds_packed, \"train_packed_alpaca.jsonl\")\n",
    "save_jsonl(eval_ds_packed, \"eval_packed_alpaca.jsonl\")\n",
    "\n",
    "\n",
    "# Create a W&B artifact\n",
    "packed_at = wandb.Artifact(\n",
    "    name=\"packed_alpaca\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Alpaca dataset packed in sequences\",\n",
    "    metadata={\"max_seq_len\":1024, \"model_id\":model_id})\n",
    "\n",
    "\n",
    "packed_at.add_file(\"train_packed_alpaca.jsonl\")\n",
    "packed_at.add_file(\"eval_packed_alpaca.jsonl\")\n",
    "\n",
    "\n",
    "# log the artifact to the project, we can give this run a job_type like `preprocess`\n",
    "with wandb.init(project=\"alpaca_ft\", job_type=\"preprocess\"):\n",
    "    wandb.log_artifact(packed_at)\n"
   ],
   "id": "6a46a4452ea87bd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/code/llm/llm-course/wandb/run-20240801_173552-6j9poxxj</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/6j9poxxj' target=\"_blank\">fragrant-feather-3</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/6j9poxxj' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/6j9poxxj</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='130.973 MB of 130.973 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f869d2fb8a094ecdbc631cee1afaef64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-feather-3</strong> at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/6j9poxxj' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/6j9poxxj</a><br/> View project at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240801_173552-6j9poxxj/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 三、用Alpaca-GPT4微调Llama2\n",
    "\n",
    "## 1. 从W&B下载之前预处理好的数据集"
   ],
   "id": "98e5284eaf186c5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:09:59.432718Z",
     "start_time": "2024-08-02T02:09:47.399832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "run = wandb.init(project=\"alpaca_ft\")\n",
    "artifact = run.use_artifact('capecape/alpaca_ft/packed_alpaca:v0', type='dataset')\n",
    "artifact_dir = Path(artifact.download())"
   ],
   "id": "3a9100331fc5bde1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mskyl4rking\u001B[0m (\u001B[33mskyl4rking-fudan-university\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113211755582598, max=1.0…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d6dca00a8274f0a8657d77f2819c6a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/liangxianbing/wandb/run-20240802_100950-7mnwm3pr</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/7mnwm3pr' target=\"_blank\">major-dust-10</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/7mnwm3pr' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/7mnwm3pr</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Downloading large artifact packed_alpaca:v0, 130.66MB. 2 files... \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   2 of 2 files downloaded.  \n",
      "Done. 0:0:1.4\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "因为我们保存的json文件，所以直接用json可以打开查看",
   "id": "893da4dcf5509f5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:10:01.437454Z",
     "start_time": "2024-08-02T02:09:59.434279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def load_jsonl(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_ds_packed = load_jsonl(artifact_dir/\"train_packed_alpaca.jsonl\")\n",
    "eval_ds_packed = load_jsonl(artifact_dir/\"eval_packed_alpaca.jsonl\")\n",
    "\n",
    "print(len(train_ds_packed[0][\"input_ids\"]), len(train_ds_packed[0][\"labels\"]))\n",
    "print(len(train_ds_packed[44][\"input_ids\"]), len(train_ds_packed[44][\"labels\"]))"
   ],
   "id": "3f7c155d7fdb212f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 1024\n",
      "1024 1024\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. 利用hugging-face Datasets从disk上加载json数据",
   "id": "c292d48f7a49c486"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:10:17.840157Z",
     "start_time": "2024-08-02T02:10:01.439524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from datasets import load_from_disk # for some reason load_dataset gives an error\n",
    "\n",
    "\n",
    "run = wandb.init(project=\"alpaca_ft\")\n",
    "artifact = run.use_artifact('capecape/alpaca_ft/packed_alpaca_hf:v0', type='dataset')\n",
    "artifact_dir = artifact.download()\n",
    "ds_packed = load_from_disk(artifact_dir)\n",
    "\n",
    "\n",
    "# we are back where we started!\n",
    "train_ds_packed = ds_packed[\"train\"]\n",
    "eval_ds_packed  = ds_packed[\"eval\"]\n",
    "max_seq_len = artifact.metadata[\"max_seq_len\"]\n"
   ],
   "id": "9f49b6705fe26a9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:7mnwm3pr) before initializing another..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8b867ef1e98434b805dce966ae5c786"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-dust-10</strong> at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/7mnwm3pr' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/7mnwm3pr</a><br/> View project at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240802_100950-7mnwm3pr/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:7mnwm3pr). Initializing new run:<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114825044448178, max=1.0…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f68ac7a2df1428385896351ecb29de8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/liangxianbing/wandb/run-20240802_101003-zgshdrm0</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/zgshdrm0' target=\"_blank\">dutiful-night-11</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/zgshdrm0' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/zgshdrm0</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Downloading large artifact packed_alpaca_hf:v0, 178.69MB. 7 files... \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   7 of 7 files downloaded.  \n",
      "Done. 0:0:1.5\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. 构建pytorch Dataloader\n",
    "- 观察数据集，input和label其实就是向后移动了一位的差距，如下图所示：  \n",
    "    <img src=./imgs/b3f82f6a.png width=auto height=200>"
   ],
   "id": "9987880a9749e773"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:10:21.530161Z",
     "start_time": "2024-08-02T02:10:17.842579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "batch_size = 8  # I have an A100 GPU with 40GB of RAM 😎\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator, # we don't need any special collator 😎\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "b = next(iter(train_dataloader))\n",
    "b.keys(), b[\"input_ids\"][0][:25], b[\"labels\"][0][:25]"
   ],
   "id": "403ae5f9d4a392f0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'labels']),\n",
       " tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937]),\n",
       " tensor([13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889, 14350,\n",
       "           263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009, 29889,\n",
       "            13,    13,  2277, 29937,  2799]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. 训练准备（函数实现与模型、训练器准备）\n",
    "- 这里只训练部分层的参数，而不是所有参数。因此某些层会被freeze\n",
    "- 这里使用了自动混合精度（[Automatic Mixed Precision](https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky)）：用了半精度浮点数，因此会加快训练\n",
    "- 梯度检查点技术（[Gradient Checkpointing](https://blog.csdn.net/Solo95/article/details/131606918)）：一部分前向激活值丢弃，另一部分保留。丢弃的部分需要计算梯度时，需要重新计算前向激活值。这样丢弃那一部分就节省了显存\n",
    "\n",
    "### (1). 构建超参数"
   ],
   "id": "8a02e92ba8284774"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:10:21.536116Z",
     "start_time": "2024-08-02T02:10:21.531621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 32 // batch_size\n",
    "\n",
    "# 超参数\n",
    "config = SimpleNamespace(\n",
    "    model_id='meta-llama/Llama-2-7b-hf',\n",
    "    dataset_name=\"alpaca-gpt4\",\n",
    "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
    "    n_freeze=24,  # How many layers we don't train, LLama 7B has 32.\n",
    "    lr=2e-4,\n",
    "    n_eval_samples=10, # How many samples to generate on validation\n",
    "    max_seq_len=max_seq_len, # Length of the sequences to pack\n",
    "    epochs=3,  # we do 3 pasess over the dataset.\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
    "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training  \n",
    "    log_model=False,  # upload the model to W&B?\n",
    "    mom=0.9, # optim param\n",
    "    gradient_checkpointing = True,  # saves even more memory\n",
    "    freeze_embed = True,  # why train this? let's keep them frozen ❄️\n",
    ")\n",
    "\n",
    "\n",
    "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps\n",
    "print(config.total_train_steps)"
   ],
   "id": "2f7fd02bb60989a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (2). 加载模型",
   "id": "8082e1e2846fe7bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:05.518475Z",
     "start_time": "2024-08-02T02:10:21.537819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    device_map=0,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    ")"
   ],
   "id": "2a08eee727070318",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "437e17edfbb948ba9eeaffc093776771"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### (3). 冻结部分参数&gradient checkpointing\n",
    "- 冻结了前面n_freeze层的transformer block\n",
    "- 一共32个llama层"
   ],
   "id": "6e95159de4f66279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:05.529179Z",
     "start_time": "2024-08-02T02:12:05.519590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)\n",
    "\n",
    "n_freeze = 30 # you can play with this parameter\n",
    "print(len(model.model.layers)) # 一共有32层\n",
    "# print(model.model.layers)\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in model.model.layers[n_freeze:].parameters(): param.requires_grad = True\n",
    "\n",
    "# Just freeze embeddings for small memory decrease\n",
    "if config.freeze_embed:\n",
    "    model.model.embed_tokens.weight.requires_grad_(False)\n",
    "    \n",
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})  # <- pytorch changed this\n",
    "    \n"
   ],
   "id": "36869253c9900188",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 6738.42M\n",
      "32\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:05.644226Z",
     "start_time": "2024-08-02T02:12:05.530613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})  # <- pytorch changed this\n",
    "    \n",
    "params, trainable_params = param_count(model)"
   ],
   "id": "57a79418aedc3d47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 535.84M\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (4). Optimizer & Scheduler",
   "id": "db823242660e119d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:06.414445Z",
     "start_time": "2024-08-02T02:12:05.645622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9,0.99), eps=1e-5)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_training_steps=config.total_train_steps,\n",
    "    num_warmup_steps=config.total_train_steps // 10,\n",
    ")\n",
    "\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n"
   ],
   "id": "8a6dfec612443de1",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (5). 测试所需函数",
   "id": "e031657108811f4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:06.660737Z",
     "start_time": "2024-08-02T02:12:06.417024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens=256,\n",
    "    gen_config=gen_config)"
   ],
   "id": "49e48c4c6e0744c6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:06.922642Z",
     "start_time": "2024-08-02T02:12:06.662168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GenerationConfig\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "\n",
    "\n",
    "def generate(prompt, max_new_tokens=100, gen_config=gen_config):\n",
    "    with torch.inference_mode():\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
    "        output = model.generate(tokenized_prompt, \n",
    "                            max_new_tokens=max_new_tokens, \n",
    "                            generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
   ],
   "id": "1fb7adad5aab1d28",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:06.928782Z",
     "start_time": "2024-08-02T02:12:06.923911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def prompt_table(examples, log=False, table_name=\"predictions\"):\n",
    "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"output\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
    "    for example in tqdm(examples, leave=False):\n",
    "        prompt, gpt4_output = example[\"prompt\"], example[\"output\"]\n",
    "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
    "        table.add_data(prompt, out, prompt+out, gpt4_output, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
    "    if log:\n",
    "        wandb.log({table_name:table})\n",
    "    return table\n",
    "\n",
    "\n",
    "def to_gpu(tensor_dict):\n",
    "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
    "\n",
    "class Accuracy:\n",
    "    \"A simple Accuracy function compatible with HF models\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.tp = 0.\n",
    "    def update(self, logits, labels):\n",
    "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
    "        tp = (logits == labels).sum()\n",
    "        self.count += len(logits)\n",
    "        self.tp += tp\n",
    "        return tp / len(logits)\n",
    "    def compute(self):\n",
    "        return self.tp / self.count"
   ],
   "id": "68fbdde3baf44ca5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:07.016106Z",
     "start_time": "2024-08-02T02:12:06.930641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval()\n",
    "    eval_acc = Accuracy()\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
    "        eval_acc.update(out.logits, batch[\"labels\"])\n",
    "    # we log results at the end\n",
    "    wandb.log({\"eval_loss\": loss.item(),\n",
    "               \"eval_accuracy\": eval_acc.compute()})\n",
    "    prompt_table(eval_dataset[:config.n_eval_samples], log=True)\n",
    "    model.train()"
   ],
   "id": "39045d6dc9efc716",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. 训练",
   "id": "1e88ae4415d678a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:07.124833Z",
     "start_time": "2024-08-02T02:12:07.017656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
    "    \"\"\"Save the model to wandb as an artifact\n",
    "    Args:\n",
    "        model (nn.Module): Model to save.\n",
    "        model_name (str): Name of the model.\n",
    "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
    "    \"\"\"\n",
    "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
    "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
    "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(file_name, safe_serialization=True)\n",
    "    # save tokenizer for easy inference\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
    "    tokenizer.save_pretrained(model_name)\n",
    "    if log:\n",
    "        at = wandb.Artifact(model_name, type=\"model\")\n",
    "        at.add_dir(file_name)\n",
    "        wandb.log_artifact(at)"
   ],
   "id": "34410c1e5beaa910",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-02T02:12:07.126604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           tags=[\"baseline\",\"7b\"],\n",
    "           job_type=\"train\",\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "\n",
    "# Training\n",
    "acc = Accuracy()\n",
    "model.train()\n",
    "train_step = 0\n",
    "pbar = tqdm(total=config.total_train_steps)\n",
    "for epoch in range(config.epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset  \n",
    "            loss.backward()\n",
    "        if step%config.gradient_accumulation_steps == 0:\n",
    "            # we can log the metrics to W&B\n",
    "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "                       \"train/accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
    "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                       \"train/global_step\": train_step})\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            train_step += 1\n",
    "            pbar.update(1)\n",
    "    validate()\n",
    "pbar.close()\n",
    "# we save the model checkpoint at the end\n",
    "save_model(\n",
    "\tmodel, \n",
    "\tmodel_name=config.model_id.replace(\"/\", \"_\"), \n",
    "\tmodels_folder=\"models/\", log=config.log_model)\n",
    "    \n",
    "wandb.finish()"
   ],
   "id": "99998bc33f94dd26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:zgshdrm0) before initializing another..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "516a2366f67b4519b320f5a581f337d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-night-11</strong> at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/zgshdrm0' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/zgshdrm0</a><br/> View project at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240802_101003-zgshdrm0/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:zgshdrm0). Initializing new run:<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112330288839681, max=1.0…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d45d0733e77d452e99c5d74525a023dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/liangxianbing/wandb/run-20240802_101207-33vvwo4g</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/33vvwo4g' target=\"_blank\">icy-waterfall-12</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/33vvwo4g' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/33vvwo4g</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                     | 0/1050 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# 测试\n",
    "with wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           job_type=\"eval\",\n",
    "           config=config): # the Hyperparameters I want to keep track of\n",
    "    model.eval()\n",
    "    prompt_table(eval_dataset[:250], log=True, table_name=\"eval_predictions\")"
   ],
   "id": "ce6a54a253e6dfa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def gpt4_judge(instruction, gen1, gen2, model=\"gpt-4\"):\n",
    "    system_prompt = (\"You will be presented with a choice of two possible responses for an instruction\"\n",
    "                     \"You have to pick the best one and give a reason why.\\n\"\n",
    "                     \"The reponse should follow the instructions and use the provided context if there is some\\n\"\n",
    "                    \"If both answers are equivalent, pick the value 0\")\n",
    "    message = \"{instruction}\\n Answer 1: \\n{gen1}\\n Answer 2:\\n{gen2}\".format(instruction=instruction, gen1=gen1, gen2=gen2)\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\",\n",
    "                   \"content\": system_prompt,\n",
    "                  },\n",
    "                  {\"role\": \"user\",\n",
    "                   \"content\": message,\n",
    "                  },],\n",
    "        function_call = {\"name\": \"make_choice\"},\n",
    "        functions = [{\n",
    "                \"name\": \"make_choice\",\n",
    "                \"description\": \"Select the best generation and explain why\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"choice\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"the choosen alternative, zero if equivalent\",\n",
    "                        },\n",
    "                        \"argument\":{\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Reason why the choice was made\",},},},\n",
    "                    \"required\": [\"choice\", \"argument\"],},\n",
    "        ],)\n",
    "    return completion\n"
   ],
   "id": "da38fc844edeab1f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

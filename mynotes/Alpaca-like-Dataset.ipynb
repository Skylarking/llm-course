{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ä¸€ã€ç®€ä»‹\n",
    "- æ•°æ®é›†æ„å»ºå‚è€ƒï¼š(https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2)\n",
    "- å¾®è°ƒå‚è€ƒï¼š(https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1)\n",
    "## 1.ä»€ä¹ˆæ˜¯Instruction Tuning\n",
    "- è¿™æ˜¯post-trainingçš„ä¸€éƒ¨åˆ†ï¼Œå³åˆ©ç”¨instructionsï¼ˆå‘½ä»¤ï¼‰æ¥å¾®è°ƒæ¨¡å‹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå’Œå¯¹é½äººç±»çš„æé—®å½¢å¼ã€‚\n",
    "- è®­ç»ƒæ–¹å¼å’Œpre-trainingæ˜¯ä¸€æ ·ï¼ŒåŒºåˆ«åœ¨äºæ•°æ®é›†çš„ä¸åŒã€‚pre-trainingæ˜¯ä¸ç”¨æ„å»ºæ•°æ®çš„ï¼Œå› ä¸ºä»–æ˜¯è‡ªç›‘ç£å­¦ä¹ ï¼ˆè‡ªå›å½’ï¼‰ã€‚ä½†æ˜¯åœ¨è¿™ä¸€æ­¥æˆ‘ä»¬éœ€è¦æ„å»ºæ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯æ„å»ºé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œè¿™ä¸€æ­¥å¯¹åé¢å®ç°chatèƒ½åŠ›è‡³å…³é‡è¦\n",
    "- æ•°æ®é›†çš„å½¢å¼ä¸ºé—®ç­”å¯¹ï¼ˆinstruction-answer pairï¼‰ï¼Œå¦‚ä¸‹ä¸¤ç§ç¤ºä¾‹:  \n",
    "\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td>\n",
    "            <font size=6> Instruction </font> <br> Explain the concept of a bubble sort algorithm to a non-technical audience.\n",
    "            </td>\n",
    "            <td>\n",
    "            <font size=6> Answer </font> <br> A bubble sort algorithm is a type of sorting algorithm that is used to sort elements in an array. It works by looking at each element of the array and comparing it to the next element. If the first element is bigger than the second element, they are swapped. This process is repeated until the whole array is sorted. This type of sorting is one of the simplest sorting algorithms, but it can be slow if the array has many elements.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "            <font size=6> Instruction </font> <br> Make the second sentence shorter. <br> <font size=6> Context </font> <br> Winter is usually the coldest season of the year. Snow is a common element during winter.\n",
    "            </td>\n",
    "            <td>\n",
    "            <font size=6> Answer </font> <br> Winter is the coldest season, often accompanied by snow.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </table>\n",
    "\n",
    "ç›®å‰å·²ç»æœ‰çš„é«˜è´¨é‡instruction datasetæœ‰æ‰‹å·¥æ„å»ºå’Œè‡ªåŠ¨æ„å»ºçš„ã€‚å…¶ä¸­åƒ[Flan Collection](https://github.com/google-research/FLAN)ï¼Œ[Dolly15k dataset](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)æ˜¯æ‰‹å·¥æ„å»ºçš„ï¼Œè€Œåƒ[Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html)æ˜¯åˆ©ç”¨LLMsæ„å»ºçš„ã€‚è¿˜æœ‰å¼€æºç¤¾åŒºæä¾›çš„ä¸€äº›æ•™å­¦æ•°æ®é›†[OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)ã€[Open-Platypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)ã€[openhermes](https://huggingface.co/datasets/teknium/openhermes)ã€‚\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨Alpaca datasetï¼ŒåŒ…æ‹¬å¦‚ä½•é¢„å¤„ç†æ•°æ®é›†ã€æ ¼å¼åŒ–ã€å’Œå¾®è°ƒLLM"
   ],
   "id": "b67580f226871b2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. ä»€ä¹ˆæ˜¯Alpaca Datasetï¼Ÿ\n",
    "è¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨OpenAI davinciæ¨¡å‹å’ŒLlamaç”Ÿæˆçš„æ•°æ®é›†ï¼Œã€‚æ‹¥æœ‰è®¸å¤šç§ç±»çš„æŒ‡ä»¤ï¼ŒåŒ…æ‹¬é‚®ä»¶å†™ä½œã€ç¤¾äº¤åª’ä½“ã€ç”Ÿäº§åŠ›å·¥å…·ã€‚æ„å»ºæ•°æ®é›†çš„pipelineå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š  \n",
    "    <img src=./imgs/f8eba1d5.png>  \n",
    "\n",
    "\n",
    "è¿™å·²ç»æ˜¯ä¸€ä¸ªè€çš„ç‰ˆæœ¬äº†ã€‚ç°åœ¨æˆ‘ä»¬ä½¿ç”¨æ›´æ–°çš„ç‰ˆæœ¬ï¼Œæ˜¯ç”¨GPT-4ç”Ÿæˆçš„ã€‚"
   ],
   "id": "d992d360abd42970"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# äºŒã€Alpaca-GPT4 Dataset\n",
    "- æ˜¯ä¸€ä¸ªjsonæ–‡ä»¶ï¼Œè¿æ¥å¦‚ä¸‹[Alpaca-GPT4 Dataset](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data.json)\n",
    "- åŒ…å«52Kæ¡æ•°æ®\n",
    "- æ˜¯é€šè¿‡GPT-4ç”Ÿæˆçš„\n",
    "\n",
    "æ ·ä¾‹æ•°æ®å¦‚ä¸‹ï¼š\n",
    "    ```\n",
    "    instruction: str, describes the task the model should perform. \n",
    "                      Each of the 52K instructions is unique.\n",
    "    input:       str, optional context or input for the task.\n",
    "    output:      str, the answer to the instruction as generated by GPT-4.\n",
    "    \n",
    "    ```\n",
    "    \n",
    "ä¸‹é¢åŠ è½½æ•°æ®é›†ï¼Œé‡‡ç”¨å°†æ•°æ®å¤¹èœæˆW&Bçš„å½¢å¼ï¼Œæ–¹ä¾¿å¿«é€ŸåŠ è½½"
   ],
   "id": "f1f4fc69046ab45b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T08:19:02.913812Z",
     "start_time": "2024-08-01T08:19:02.875569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# è·å–å½“å‰å·¥ä½œç›®å½•\n",
    "current_directory = os.getcwd()\n",
    "print(\"å½“å‰å·¥ä½œç›®å½•:\", current_directory)\n",
    "\n",
    "# è®¾ç½®æ–°çš„å·¥ä½œç›®å½•\n",
    "new_directory = \"/mnt/d/lib/llm/llm-course\"\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# å†æ¬¡è·å–å½“å‰å·¥ä½œç›®å½•ï¼Œç¡®è®¤æ˜¯å¦æ›´æ”¹æˆåŠŸ\n",
    "current_directory = os.getcwd()\n",
    "print(\"æ–°çš„å·¥ä½œç›®å½•:\", current_directory)"
   ],
   "id": "b87be3ed5a34dc7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰å·¥ä½œç›®å½•: /home/liangxianbing\n",
      "æ–°çš„å·¥ä½œç›®å½•: /mnt/d/code/llm/llm-course\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T08:53:39.204493Z",
     "start_time": "2024-08-01T08:53:25.299302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import wandb\n",
    "\n",
    "\n",
    "with open(\"./mynotes/data/alpaca_dataset/alpaca_gpt4_data.json\", \"r\") as f:\n",
    "    alpaca = json.load(f)\n",
    "\n",
    "\n",
    "with wandb.init(project=\"alpaca_ft\"):\n",
    "    at = wandb.Artifact(\n",
    "        name=\"alpaca_gpt4\", \n",
    "        type=\"dataset\",\n",
    "        description=\"A GPT4 generated Alpaca like dataset for instruction finetunning\",\n",
    "        metadata={\"url\":\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\"},\n",
    "    )\n",
    "    at.add_file(\"./mynotes/data/alpaca_dataset/alpaca_gpt4_data.json\")\n",
    "    \n",
    "    # table\n",
    "    table = wandb.Table(columns=list(alpaca[0].keys()))\n",
    "    for row in alpaca:\n",
    "        table.add_data(*row.values())"
   ],
   "id": "6e8c971cc4b61bb8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mskyl4rking\u001B[0m (\u001B[33mskyl4rking-fudan-university\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/code/llm/llm-course/wandb/run-20240801_165326-sk7zc6r9</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/sk7zc6r9' target=\"_blank\">hopeful-darkness-1</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/sk7zc6r9' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/sk7zc6r9</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac91a38675f648fca8ea41119a22dd88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hopeful-darkness-1</strong> at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/sk7zc6r9' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/sk7zc6r9</a><br/> View project at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240801_165326-sk7zc6r9/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.Tokenization",
   "id": "45fc32f34c40e6f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T08:57:01.135293Z",
     "start_time": "2024-08-01T08:57:00.858197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"./mynotes/data/alpaca_dataset/alpaca_gpt4_data.json\", \"r\") as f:\n",
    "    alpaca = json.load(f)\n",
    "\n",
    "\n",
    "print(len(alpaca))\n",
    "\n",
    "\n",
    "one_row = alpaca[232]\n",
    "print(one_row)"
   ],
   "id": "fecec7f9ab451335",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52002\n",
      "{'instruction': 'Sort the following list in alphabetical order.', 'input': 'Camouflage, Furniture, Plaster', 'output': 'Camouflage, Furniture, Plaster sorted in alphabetical order:\\nCamouflage, Furniture, Plaster'}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (1). å®šä¹‰å°†æ•°æ®æ ¼å¼åŒ–çš„å‡½æ•°ï¼ˆå°±æ˜¯ç”Ÿæˆpromptï¼‰",
   "id": "d3ff5fdbfea1953d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T08:59:50.123699Z",
     "start_time": "2024-08-01T08:59:50.119642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "\n",
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "row = alpaca[232]\n",
    "print(prompt_input(row))"
   ],
   "id": "c75ffdd43a886f12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (2). æ„å»ºæ‰€æœ‰æ•°æ®ç”Ÿæˆpromptçš„å‡½æ•°",
   "id": "e8e6afe7255c993f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:00:25.183685Z",
     "start_time": "2024-08-01T09:00:25.148003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_prompt(row):\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)\n",
    "\n",
    "\n",
    "prompts = [create_prompt(row) for row in alpaca]  # all LLM inputs are here\n",
    "print(prompts[232])"
   ],
   "id": "40a0c718c705fc8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (3). å®šä¹‰è¾“å‡ºï¼ˆå³labelï¼‰ï¼Œåªéœ€è¦æé‚£å®¶ä¸€ä¸ªeoså³å¯",
   "id": "6ec18274124c7b40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:02:21.941568Z",
     "start_time": "2024-08-01T09:02:21.918722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EOS_TOKEN = \"</s>\"\n",
    "outputs = [row['output'] + EOS_TOKEN for row in alpaca]\n",
    "print(outputs[232])"
   ],
   "id": "b660cc7bdbc15144",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camouflage, Furniture, Plaster sorted in alphabetical order:\n",
      "Camouflage, Furniture, Plaster</s>\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (4).ä¿å­˜æ•°æ®é›†",
   "id": "689681a0c80eb9fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:04:29.809239Z",
     "start_time": "2024-08-01T09:04:29.774012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = [{\"prompt\":s, \"output\":t, \"example\": s+t} for s, t in zip(prompts, outputs)]\n",
    "print(dataset[232]['prompt'])\n",
    "print('____________________________________________________________________________________________')\n",
    "print(dataset[232]['output'])\n",
    "print('____________________________________________________________________________________________')\n",
    "print(dataset[232]['example'])"
   ],
   "id": "c692a8118f5a56d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "\n",
      "____________________________________________________________________________________________\n",
      "Camouflage, Furniture, Plaster sorted in alphabetical order:\n",
      "Camouflage, Furniture, Plaster</s>\n",
      "____________________________________________________________________________________________\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "Camouflage, Furniture, Plaster sorted in alphabetical order:\n",
      "Camouflage, Furniture, Plaster</s>\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### (5). è¿›è¡ŒTokenizer\n",
    "- å¯ä»¥çœ‹åˆ°padäº†3ä¸ª2"
   ],
   "id": "6d3df24b5688b608"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:16:48.939641Z",
     "start_time": "2024-08-01T09:16:48.624535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.encode(\"My experiments are going strong!\", padding='max_length', max_length=10)\n",
    "\n",
    "# ä¹Ÿå¯ä»¥ç›´æ¥å˜æˆtorch Tensor\n",
    "# tokenizer.encode(\"My experiments are going strong!\", \n",
    "#                  padding='max_length', \n",
    "#                  max_length=10,\n",
    "#                  return_tensors=\"pt\")"
   ],
   "id": "35af52963b3385f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1619, 15729, 526, 2675, 4549, 29991, 2, 2, 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. å¤„ç†æˆæ•°æ®é›†ï¼Œå¹¶ä¸Šä¼ åˆ°Wandb\n",
    "### (1). åˆ‡éªŒè¯é›†å’Œè®­ç»ƒé›†"
   ],
   "id": "8bf5868bc8019f95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:20:38.689143Z",
     "start_time": "2024-08-01T09:19:56.298086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "random.shuffle(dataset)  # shuffle inplace\n",
    "\n",
    "\n",
    "train_dataset = dataset[:-1000]\n",
    "eval_dataset = dataset[-1000:]\n",
    "\n",
    "\n",
    "train_table = wandb.Table(dataframe=pd.DataFrame(train_dataset))\n",
    "eval_table  = wandb.Table(dataframe=pd.DataFrame(eval_dataset))\n",
    "\n",
    "\n",
    "with wandb.init(project=\"alpaca_ft\", job_type=\"split_data\"):\n",
    "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})\n"
   ],
   "id": "d52978b8a0c7cda9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/code/llm/llm-course/wandb/run-20240801_171959-jrw3yxl6</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/jrw3yxl6' target=\"_blank\">lyric-fog-2</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/jrw3yxl6' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/jrw3yxl6</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='114.333 MB of 114.344 MB uploaded\\r'), FloatProgress(value=0.9999024004070258, maxâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f35d00e0e544a2a912fc40969cf2744"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-fog-2</strong> at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/jrw3yxl6' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/jrw3yxl6</a><br/> View project at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240801_171959-jrw3yxl6/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### æ–¹æ¡ˆä¸€ï¼šå°†å¤šä¸ªæ ·æœ¬ç»„åˆæˆä¸€ä¸ªé•¿åºåˆ—\n",
    "é•¿åºåˆ—å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå°†å¤šä¸ªæ ·æœ¬å˜æˆä¸€ä¸ªå«æœ‰1kçš„é•¿åºåˆ—\n",
    "<img src=./imgs/d9f4c0c2.png width=auto height=200>"
   ],
   "id": "cb721f0302cd5b0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:26:48.181417Z",
     "start_time": "2024-08-01T09:26:43.812849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_seq_len = 1024\n",
    "\n",
    "\n",
    "def pack(dataset, max_seq_len=1024):\n",
    "    tkds_ids = tokenizer([s[\"example\"] for s in dataset])[\"input_ids\"]  # è·å–æ‰€æœ‰æ ·æœ¬çš„token_id\n",
    "    \n",
    "    # å°†datasetä¸­æ‰€æœ‰æ ·æœ¬token_idæ‹¼æ¥åˆ°ä¸€èµ·\n",
    "    all_token_ids = []\n",
    "    for tokenized_input in tkds_ids:\n",
    "        all_token_ids.extend(tokenized_input + [tokenizer.eos_token_id])    \n",
    "    \n",
    "    # æ¯æ®µåˆ‡1024ä¸ªtokenï¼Œå¹¶ç”Ÿæˆå¯¹åº”label\n",
    "    packed_ds = []\n",
    "    for i in range(0, len(all_token_ids), max_seq_len+1):\n",
    "        input_ids = all_token_ids[i : i + max_seq_len+1]\n",
    "        if len(input_ids) == (max_seq_len+1):\n",
    "            packed_ds.append({\"input_ids\": input_ids[:-1], \"labels\": input_ids[1:]})  # < --- â€¼ï¸ â›”ï¸\n",
    "\t    # if you use the model.output.loss you don't need to shift, it is done for you!\n",
    "    return packed_ds\n",
    "\n",
    "\n",
    "train_ds_packed = pack(train_dataset)\n",
    "eval_ds_packed = pack(eval_dataset)\n",
    "print(len(train_ds_packed[0][\"input_ids\"]), len(train_ds_packed[0][\"labels\"]))\n",
    "print(len(train_ds_packed[44][\"input_ids\"]), len(train_ds_packed[44][\"labels\"]))"
   ],
   "id": "f35016a1b2b61dc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 1024\n",
      "1024 1024\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### æ–¹æ¡ˆäºŒï¼šå°†æ¯ä¸ªæ ·æœ¬padæˆåŒæ ·é•¿åº¦\n",
    "- å¯ä»¥é€‰æ‹©æœ€é•¿é•¿åº¦pad\n",
    "- æˆ–è€…é€‰æ‹©ä¸€ä¸ªæœ€å¤§å€¼æ¥pad\n",
    "ä¸¤ç§æ–¹æ¡ˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š  \n",
    "<img src=./imgs/ff512cfb.png width=auto height=200>"
   ],
   "id": "aba42ff2b8cf6cc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:34:13.366153Z",
     "start_time": "2024-08-01T09:34:13.360922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = tokenizer([\"My experiments are going strong!\", \n",
    "           \"I love Llamas\"], \n",
    "          padding='longest',\n",
    "          return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "b = tokenizer([\"My experiments are going strong!\", \n",
    "           \"I love Llamas\"], \n",
    "          # padding='max_length', \n",
    "          padding='max_length',\n",
    "          max_length=10,\n",
    "          return_tensors=\"pt\")\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ],
   "id": "d7ef51e4d83e9ba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  1619, 15729,   526,  2675,  4549, 29991],\n",
      "        [    1,   306,  5360,   365,  5288,   294,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n",
      "{'input_ids': tensor([[    1,  1619, 15729,   526,  2675,  4549, 29991,     2,     2,     2],\n",
      "        [    1,   306,  5360,   365,  5288,   294,     2,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (2). ä¿å­˜æ•°æ®åˆ°W&Bï¼ˆæ–¹æ¡ˆä¸€ï¼‰",
   "id": "367e79605021ad25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T09:36:28.267918Z",
     "start_time": "2024-08-01T09:35:40.304313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "def save_jsonl(data, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for entry in data:\n",
    "            json.dump(entry, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "# dump everything to jsonl files\n",
    "save_jsonl(train_ds_packed, \"train_packed_alpaca.jsonl\")\n",
    "save_jsonl(eval_ds_packed, \"eval_packed_alpaca.jsonl\")\n",
    "\n",
    "\n",
    "# Create a W&B artifact\n",
    "packed_at = wandb.Artifact(\n",
    "    name=\"packed_alpaca\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Alpaca dataset packed in sequences\",\n",
    "    metadata={\"max_seq_len\":1024, \"model_id\":model_id})\n",
    "\n",
    "\n",
    "packed_at.add_file(\"train_packed_alpaca.jsonl\")\n",
    "packed_at.add_file(\"eval_packed_alpaca.jsonl\")\n",
    "\n",
    "\n",
    "# log the artifact to the project, we can give this run a job_type like `preprocess`\n",
    "with wandb.init(project=\"alpaca_ft\", job_type=\"preprocess\"):\n",
    "    wandb.log_artifact(packed_at)\n"
   ],
   "id": "6a46a4452ea87bd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/code/llm/llm-course/wandb/run-20240801_173552-6j9poxxj</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/6j9poxxj' target=\"_blank\">fragrant-feather-3</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/6j9poxxj' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/6j9poxxj</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='130.973 MB of 130.973 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f869d2fb8a094ecdbc631cee1afaef64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-feather-3</strong> at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/6j9poxxj' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/6j9poxxj</a><br/> View project at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240801_173552-6j9poxxj/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ä¸‰ã€ç”¨Alpaca-GPT4å¾®è°ƒLlama2\n",
    "\n",
    "## 1. ä»W&Bä¸‹è½½ä¹‹å‰é¢„å¤„ç†å¥½çš„æ•°æ®é›†"
   ],
   "id": "98e5284eaf186c5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:09:59.432718Z",
     "start_time": "2024-08-02T02:09:47.399832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "run = wandb.init(project=\"alpaca_ft\")\n",
    "artifact = run.use_artifact('capecape/alpaca_ft/packed_alpaca:v0', type='dataset')\n",
    "artifact_dir = Path(artifact.download())"
   ],
   "id": "3a9100331fc5bde1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mskyl4rking\u001B[0m (\u001B[33mskyl4rking-fudan-university\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113211755582598, max=1.0â€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d6dca00a8274f0a8657d77f2819c6a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/liangxianbing/wandb/run-20240802_100950-7mnwm3pr</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/7mnwm3pr' target=\"_blank\">major-dust-10</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/7mnwm3pr' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/7mnwm3pr</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Downloading large artifact packed_alpaca:v0, 130.66MB. 2 files... \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   2 of 2 files downloaded.  \n",
      "Done. 0:0:1.4\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "å› ä¸ºæˆ‘ä»¬ä¿å­˜çš„jsonæ–‡ä»¶ï¼Œæ‰€ä»¥ç›´æ¥ç”¨jsonå¯ä»¥æ‰“å¼€æŸ¥çœ‹",
   "id": "893da4dcf5509f5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:10:01.437454Z",
     "start_time": "2024-08-02T02:09:59.434279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def load_jsonl(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_ds_packed = load_jsonl(artifact_dir/\"train_packed_alpaca.jsonl\")\n",
    "eval_ds_packed = load_jsonl(artifact_dir/\"eval_packed_alpaca.jsonl\")\n",
    "\n",
    "print(len(train_ds_packed[0][\"input_ids\"]), len(train_ds_packed[0][\"labels\"]))\n",
    "print(len(train_ds_packed[44][\"input_ids\"]), len(train_ds_packed[44][\"labels\"]))"
   ],
   "id": "3f7c155d7fdb212f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 1024\n",
      "1024 1024\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. åˆ©ç”¨hugging-face Datasetsä»diskä¸ŠåŠ è½½jsonæ•°æ®",
   "id": "c292d48f7a49c486"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:10:17.840157Z",
     "start_time": "2024-08-02T02:10:01.439524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from datasets import load_from_disk # for some reason load_dataset gives an error\n",
    "\n",
    "\n",
    "run = wandb.init(project=\"alpaca_ft\")\n",
    "artifact = run.use_artifact('capecape/alpaca_ft/packed_alpaca_hf:v0', type='dataset')\n",
    "artifact_dir = artifact.download()\n",
    "ds_packed = load_from_disk(artifact_dir)\n",
    "\n",
    "\n",
    "# we are back where we started!\n",
    "train_ds_packed = ds_packed[\"train\"]\n",
    "eval_ds_packed  = ds_packed[\"eval\"]\n",
    "max_seq_len = artifact.metadata[\"max_seq_len\"]\n"
   ],
   "id": "9f49b6705fe26a9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:7mnwm3pr) before initializing another..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8b867ef1e98434b805dce966ae5c786"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-dust-10</strong> at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/7mnwm3pr' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/7mnwm3pr</a><br/> View project at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240802_100950-7mnwm3pr/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:7mnwm3pr). Initializing new run:<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114825044448178, max=1.0â€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f68ac7a2df1428385896351ecb29de8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/liangxianbing/wandb/run-20240802_101003-zgshdrm0</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/zgshdrm0' target=\"_blank\">dutiful-night-11</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/zgshdrm0' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/zgshdrm0</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Downloading large artifact packed_alpaca_hf:v0, 178.69MB. 7 files... \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   7 of 7 files downloaded.  \n",
      "Done. 0:0:1.5\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. æ„å»ºpytorch Dataloader\n",
    "- è§‚å¯Ÿæ•°æ®é›†ï¼Œinputå’Œlabelå…¶å®å°±æ˜¯å‘åç§»åŠ¨äº†ä¸€ä½çš„å·®è·ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š  \n",
    "    <img src=./imgs/b3f82f6a.png width=auto height=200>"
   ],
   "id": "9987880a9749e773"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:10:21.530161Z",
     "start_time": "2024-08-02T02:10:17.842579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "batch_size = 8  # I have an A100 GPU with 40GB of RAM ğŸ˜\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator, # we don't need any special collator ğŸ˜\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "b = next(iter(train_dataloader))\n",
    "b.keys(), b[\"input_ids\"][0][:25], b[\"labels\"][0][:25]"
   ],
   "id": "403ae5f9d4a392f0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'labels']),\n",
       " tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937]),\n",
       " tensor([13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889, 14350,\n",
       "           263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009, 29889,\n",
       "            13,    13,  2277, 29937,  2799]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. è®­ç»ƒå‡†å¤‡ï¼ˆå‡½æ•°å®ç°ä¸æ¨¡å‹ã€è®­ç»ƒå™¨å‡†å¤‡ï¼‰\n",
    "- è¿™é‡Œåªè®­ç»ƒéƒ¨åˆ†å±‚çš„å‚æ•°ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å‚æ•°ã€‚å› æ­¤æŸäº›å±‚ä¼šè¢«freeze\n",
    "- è¿™é‡Œä½¿ç”¨äº†è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆ[Automatic Mixed Precision](https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky)ï¼‰ï¼šç”¨äº†åŠç²¾åº¦æµ®ç‚¹æ•°ï¼Œå› æ­¤ä¼šåŠ å¿«è®­ç»ƒ\n",
    "- æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼ˆ[Gradient Checkpointing](https://blog.csdn.net/Solo95/article/details/131606918)ï¼‰ï¼šä¸€éƒ¨åˆ†å‰å‘æ¿€æ´»å€¼ä¸¢å¼ƒï¼Œå¦ä¸€éƒ¨åˆ†ä¿ç•™ã€‚ä¸¢å¼ƒçš„éƒ¨åˆ†éœ€è¦è®¡ç®—æ¢¯åº¦æ—¶ï¼Œéœ€è¦é‡æ–°è®¡ç®—å‰å‘æ¿€æ´»å€¼ã€‚è¿™æ ·ä¸¢å¼ƒé‚£ä¸€éƒ¨åˆ†å°±èŠ‚çœäº†æ˜¾å­˜\n",
    "\n",
    "### (1). æ„å»ºè¶…å‚æ•°"
   ],
   "id": "8a02e92ba8284774"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:10:21.536116Z",
     "start_time": "2024-08-02T02:10:21.531621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 32 // batch_size\n",
    "\n",
    "# è¶…å‚æ•°\n",
    "config = SimpleNamespace(\n",
    "    model_id='meta-llama/Llama-2-7b-hf',\n",
    "    dataset_name=\"alpaca-gpt4\",\n",
    "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
    "    n_freeze=24,  # How many layers we don't train, LLama 7B has 32.\n",
    "    lr=2e-4,\n",
    "    n_eval_samples=10, # How many samples to generate on validation\n",
    "    max_seq_len=max_seq_len, # Length of the sequences to pack\n",
    "    epochs=3,  # we do 3 pasess over the dataset.\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
    "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training  \n",
    "    log_model=False,  # upload the model to W&B?\n",
    "    mom=0.9, # optim param\n",
    "    gradient_checkpointing = True,  # saves even more memory\n",
    "    freeze_embed = True,  # why train this? let's keep them frozen â„ï¸\n",
    ")\n",
    "\n",
    "\n",
    "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps\n",
    "print(config.total_train_steps)"
   ],
   "id": "2f7fd02bb60989a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (2). åŠ è½½æ¨¡å‹",
   "id": "8082e1e2846fe7bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:05.518475Z",
     "start_time": "2024-08-02T02:10:21.537819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    device_map=0,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    ")"
   ],
   "id": "2a08eee727070318",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "437e17edfbb948ba9eeaffc093776771"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### (3). å†»ç»“éƒ¨åˆ†å‚æ•°&gradient checkpointing\n",
    "- å†»ç»“äº†å‰é¢n_freezeå±‚çš„transformer block\n",
    "- ä¸€å…±32ä¸ªllamaå±‚"
   ],
   "id": "6e95159de4f66279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:05.529179Z",
     "start_time": "2024-08-02T02:12:05.519590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)\n",
    "\n",
    "n_freeze = 30 # you can play with this parameter\n",
    "print(len(model.model.layers)) # ä¸€å…±æœ‰32å±‚\n",
    "# print(model.model.layers)\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in model.model.layers[n_freeze:].parameters(): param.requires_grad = True\n",
    "\n",
    "# Just freeze embeddings for small memory decrease\n",
    "if config.freeze_embed:\n",
    "    model.model.embed_tokens.weight.requires_grad_(False)\n",
    "    \n",
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})  # <- pytorch changed this\n",
    "    \n"
   ],
   "id": "36869253c9900188",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 6738.42M\n",
      "32\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:05.644226Z",
     "start_time": "2024-08-02T02:12:05.530613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})  # <- pytorch changed this\n",
    "    \n",
    "params, trainable_params = param_count(model)"
   ],
   "id": "57a79418aedc3d47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 535.84M\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (4). Optimizer & Scheduler",
   "id": "db823242660e119d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:06.414445Z",
     "start_time": "2024-08-02T02:12:05.645622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9,0.99), eps=1e-5)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_training_steps=config.total_train_steps,\n",
    "    num_warmup_steps=config.total_train_steps // 10,\n",
    ")\n",
    "\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n"
   ],
   "id": "8a6dfec612443de1",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (5). æµ‹è¯•æ‰€éœ€å‡½æ•°",
   "id": "e031657108811f4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:06.660737Z",
     "start_time": "2024-08-02T02:12:06.417024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens=256,\n",
    "    gen_config=gen_config)"
   ],
   "id": "49e48c4c6e0744c6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:06.922642Z",
     "start_time": "2024-08-02T02:12:06.662168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GenerationConfig\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "\n",
    "\n",
    "def generate(prompt, max_new_tokens=100, gen_config=gen_config):\n",
    "    with torch.inference_mode():\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
    "        output = model.generate(tokenized_prompt, \n",
    "                            max_new_tokens=max_new_tokens, \n",
    "                            generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
   ],
   "id": "1fb7adad5aab1d28",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:06.928782Z",
     "start_time": "2024-08-02T02:12:06.923911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def prompt_table(examples, log=False, table_name=\"predictions\"):\n",
    "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"output\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
    "    for example in tqdm(examples, leave=False):\n",
    "        prompt, gpt4_output = example[\"prompt\"], example[\"output\"]\n",
    "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
    "        table.add_data(prompt, out, prompt+out, gpt4_output, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
    "    if log:\n",
    "        wandb.log({table_name:table})\n",
    "    return table\n",
    "\n",
    "\n",
    "def to_gpu(tensor_dict):\n",
    "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
    "\n",
    "class Accuracy:\n",
    "    \"A simple Accuracy function compatible with HF models\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.tp = 0.\n",
    "    def update(self, logits, labels):\n",
    "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
    "        tp = (logits == labels).sum()\n",
    "        self.count += len(logits)\n",
    "        self.tp += tp\n",
    "        return tp / len(logits)\n",
    "    def compute(self):\n",
    "        return self.tp / self.count"
   ],
   "id": "68fbdde3baf44ca5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:07.016106Z",
     "start_time": "2024-08-02T02:12:06.930641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval()\n",
    "    eval_acc = Accuracy()\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
    "        eval_acc.update(out.logits, batch[\"labels\"])\n",
    "    # we log results at the end\n",
    "    wandb.log({\"eval_loss\": loss.item(),\n",
    "               \"eval_accuracy\": eval_acc.compute()})\n",
    "    prompt_table(eval_dataset[:config.n_eval_samples], log=True)\n",
    "    model.train()"
   ],
   "id": "39045d6dc9efc716",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. è®­ç»ƒ",
   "id": "1e88ae4415d678a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T02:12:07.124833Z",
     "start_time": "2024-08-02T02:12:07.017656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
    "    \"\"\"Save the model to wandb as an artifact\n",
    "    Args:\n",
    "        model (nn.Module): Model to save.\n",
    "        model_name (str): Name of the model.\n",
    "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
    "    \"\"\"\n",
    "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
    "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
    "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(file_name, safe_serialization=True)\n",
    "    # save tokenizer for easy inference\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
    "    tokenizer.save_pretrained(model_name)\n",
    "    if log:\n",
    "        at = wandb.Artifact(model_name, type=\"model\")\n",
    "        at.add_dir(file_name)\n",
    "        wandb.log_artifact(at)"
   ],
   "id": "34410c1e5beaa910",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-02T02:12:07.126604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           tags=[\"baseline\",\"7b\"],\n",
    "           job_type=\"train\",\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "\n",
    "# Training\n",
    "acc = Accuracy()\n",
    "model.train()\n",
    "train_step = 0\n",
    "pbar = tqdm(total=config.total_train_steps)\n",
    "for epoch in range(config.epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset  \n",
    "            loss.backward()\n",
    "        if step%config.gradient_accumulation_steps == 0:\n",
    "            # we can log the metrics to W&B\n",
    "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "                       \"train/accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
    "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                       \"train/global_step\": train_step})\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            train_step += 1\n",
    "            pbar.update(1)\n",
    "    validate()\n",
    "pbar.close()\n",
    "# we save the model checkpoint at the end\n",
    "save_model(\n",
    "\tmodel, \n",
    "\tmodel_name=config.model_id.replace(\"/\", \"_\"), \n",
    "\tmodels_folder=\"models/\", log=config.log_model)\n",
    "    \n",
    "wandb.finish()"
   ],
   "id": "99998bc33f94dd26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:zgshdrm0) before initializing another..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "516a2366f67b4519b320f5a581f337d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-night-11</strong> at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/zgshdrm0' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/zgshdrm0</a><br/> View project at: <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240802_101003-zgshdrm0/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:zgshdrm0). Initializing new run:<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112330288839681, max=1.0â€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d45d0733e77d452e99c5d74525a023dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/liangxianbing/wandb/run-20240802_101207-33vvwo4g</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/33vvwo4g' target=\"_blank\">icy-waterfall-12</a></strong> to <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/33vvwo4g' target=\"_blank\">https://wandb.ai/skyl4rking-fudan-university/alpaca_ft/runs/33vvwo4g</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                     | 0/1050 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# æµ‹è¯•\n",
    "with wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           job_type=\"eval\",\n",
    "           config=config): # the Hyperparameters I want to keep track of\n",
    "    model.eval()\n",
    "    prompt_table(eval_dataset[:250], log=True, table_name=\"eval_predictions\")"
   ],
   "id": "ce6a54a253e6dfa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def gpt4_judge(instruction, gen1, gen2, model=\"gpt-4\"):\n",
    "    system_prompt = (\"You will be presented with a choice of two possible responses for an instruction\"\n",
    "                     \"You have to pick the best one and give a reason why.\\n\"\n",
    "                     \"The reponse should follow the instructions and use the provided context if there is some\\n\"\n",
    "                    \"If both answers are equivalent, pick the value 0\")\n",
    "    message = \"{instruction}\\n Answer 1: \\n{gen1}\\n Answer 2:\\n{gen2}\".format(instruction=instruction, gen1=gen1, gen2=gen2)\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\",\n",
    "                   \"content\": system_prompt,\n",
    "                  },\n",
    "                  {\"role\": \"user\",\n",
    "                   \"content\": message,\n",
    "                  },],\n",
    "        function_call = {\"name\": \"make_choice\"},\n",
    "        functions = [{\n",
    "                \"name\": \"make_choice\",\n",
    "                \"description\": \"Select the best generation and explain why\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"choice\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"the choosen alternative, zero if equivalent\",\n",
    "                        },\n",
    "                        \"argument\":{\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Reason why the choice was made\",},},},\n",
    "                    \"required\": [\"choice\", \"argument\"],},\n",
    "        ],)\n",
    "    return completion\n"
   ],
   "id": "da38fc844edeab1f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
